{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635a938e",
   "metadata": {},
   "source": [
    "## steps i will follow\n",
    "- learn to load a model locally give a prompt to and check how it gives output\n",
    "- design a pipeline  \n",
    "- design a prompt \n",
    "- storing the answer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eec8b4e",
   "metadata": {},
   "source": [
    "let's us check the our model compatible or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dbd6367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.99951171875"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "gpu_mem=torch.cuda.get_device_properties(0).total_memory\n",
    "gpu_mem=gpu_mem/2**30\n",
    "gpu_mem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e129e1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jun 14 04:44:49 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 576.57                 Driver Version: 576.57         CUDA Version: 12.9     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 2050      WDDM  |   00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   32C    P0             10W /   70W |       0MiB /   4096MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff41217",
   "metadata": {},
   "source": [
    "### get access of gemma model 1b it \n",
    "- https://huggingface.co/google/gemma-3-1b-it\n",
    "\n",
    "### download the hugging face cli to directly login from terminal \n",
    "- https://huggingface.co/docs/huggingface_hub/en/guides/cli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3066ca0",
   "metadata": {},
   "source": [
    "## requirement for flash attention 2 \n",
    "\n",
    "- Requiremennt------- Minimum Version ---------------------mine version\n",
    "- CUDA\t      ------- 11.6 or later- ----------------------- 12.9\n",
    "- PyTorch     ------- 2.0 or later --------------------------- 2.7.0+cu118\n",
    "- NVIDIA GPU ------- Ampere or newer (A100, A10, H100, etc.)------rtx 2050\n",
    "- Python\t------- 3.8+       ---------------------------  3.12.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7349830f",
   "metadata": {},
   "source": [
    "## installing flash attention for attention architecture\n",
    "\n",
    "- https://github.com/Dao-AILab/flash-attention\n",
    "-  Make sure that ninja is installed and that it works correctly ---->given in above github link req for installation of flask \n",
    "- will take around 8 hour to compile \n",
    "- version   flash_attn-2.5.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "521f1403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ALOK\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26997fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 2050\n",
      "(8, 6)\n",
      "_CudaDeviceProperties(name='NVIDIA GeForce RTX 2050', major=8, minor=6, total_memory=4095MB, multi_processor_count=16, uuid=e4e4042f-c536-1e4f-3663-67440a04ba8f, L2_cache_size=1MB)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_capability(0))#------>(8,6) this means it is compute capability is 8.6\n",
    "print(torch.cuda.get_device_properties(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a3f423",
   "metadata": {},
   "source": [
    "## error i facead while loading this model\n",
    "- use this specific transformer model ------------> pip install git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3 <br>\n",
    "  even latest transformer model did not suppport soo use this only\n",
    "- i was accidentally trying to load 2b model and my kernel crashed\n",
    "\n",
    "- if u are using quantization config then do not set low_cpu_mem_usage=False ----> in llm_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf32f49",
   "metadata": {},
   "source": [
    "the reason i am not using quantization config cause <br>\n",
    "model is ending up on the CPU, but i am trying to use fp4 quantization, which is only supported on GPU by the bitsandbytes library.<br>\n",
    "but since this bits and and bytes i am using is not cuda supported cause my cuda version is 12.9.<br>\n",
    ">version is newer than what bitsandbytes officially supports. The latest prebuilt bitsandbytes wheel currently supports up to CUDA 12.0 — \n",
    ">there is no official bitsandbytes-cuda129 wheel yet.\n",
    "\n",
    "> i have have a CPU-only version of bitsandbytes installed — which does not support GPU quantization types like fp4 or int4.\n",
    "\n",
    "### i tried above by making use_quantization false but geting this error\n",
    "-Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument index in method<br>        wrapper_CUDA__index_select)\n",
    " \n",
    "- i am running outof gpu memory that i suppose cause i tried all possible way to but failed .(btw my both input id and llm model is on cuda stil this error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "614f6fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "quantization_config= BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_compute_dtype=torch.float16)\n",
    "use_quantization_config=True\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "attention_implementation= 'flash_attention_2'\n",
    "model_id='google/gemma-3-1b-it'\n",
    "tokenizer=AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "llm_model=AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                        torch_dtype=torch.float16,\n",
    "                                        quantization_config=quantization_config if use_quantization_config else None,\n",
    "                                        attn_implementation=attention_implementation)\n",
    "if not quantization_config:\n",
    "    llm_model.to('device')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a9570e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cb455f4",
   "metadata": {},
   "source": [
    "## below to check flash attention 2 is used or not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa95205e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flash_attention_2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(llm_model.config._attn_implementation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f6437aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma3ForCausalLM(\n",
       "  (model): Gemma3TextModel(\n",
       "    (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-25): 26 x Gemma3DecoderLayer(\n",
       "        (self_attn): Gemma3Attention(\n",
       "          (q_proj): Linear4bit(in_features=1152, out_features=1024, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=1152, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=1152, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=1024, out_features=1152, bias=False)\n",
       "          (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "          (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "        )\n",
       "        (mlp): Gemma3MLP(\n",
       "          (gate_proj): Linear4bit(in_features=1152, out_features=6912, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=1152, out_features=6912, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=6912, out_features=1152, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "    (rotary_emb): Gemma3RotaryEmbedding()\n",
       "    (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1152, out_features=262144, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439b40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(r\"D:\\summer project\\Fire\\final_dataset\\twittter_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4c55f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for pos,row in df.iterrows():\n",
    "    data.append(row.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "aa501d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def promt_generator(num:int,data:list[dict]):\n",
    "    tweet=data[num][\"cleaned_tweet\"]\n",
    "    language=data[num][\"language\"]\n",
    "    label_1=data[num][\"L1_Datatype\"]\n",
    "    label_2=data[num][\"L2_Datatype\"]\n",
    "    label_3=data[num][\"L3_Datatype\"]\n",
    "    label_final=data[num][\"Datatype\"]\n",
    "\n",
    "    base_prompt='''\n",
    "you have specialized at detecting sentence emotion.we are playing a game in which i will give you a sentence and it's suggested emotion which we call datatype . datatype \n",
    "will be analyzed at 3 level and you have to anlyze it emotion as per suggested emotion at each level and has to respond strictly in binary whether you Agree or Disagree\n",
    "CRITICAL INSTRUCTIONS:\n",
    "- OUTPUT ONLY: \"Agree\" or \"Disagree\"\n",
    "- NO EXPLANATIONS: Do not provide reasoning or additional text\n",
    "- you have to strictly give some output and cannot leave output blank \n",
    "- strictly analyze each level and if you Disagree at any level give response in binary as disagree and if you agree at each level respond agree\\n\n",
    "\n",
    "CLASSIFICATION HIERARCHY:\n",
    "Level 1 (Primary Classification) - MANDATORY\n",
    "- NOISE: Unintelligible text, random characters, excessive symbols, spam, gibberish\n",
    "- OBJECTIVE: Factual statements, news reports, informational content without personal opinion\n",
    "- SUBJECTIVE: Personal opinions, emotions, feelings, subjective experiences\\n\n",
    "\n",
    "Level 2 (Sentiment Classification) - ONLY if Level 1=SUBJECTIVE\n",
    "-  NEUTRAL: No clear positive/negative sentiment, balanced opinion, uncertainty\n",
    "-  NEGATIVE: Criticism, complaints, dissatisfaction, negative emotions\n",
    "- POSITIVE: Praise, satisfaction, positive emotions, recommendations\n",
    "\n",
    "Level 3 (Neutral Subcategories) - ONLY if Level 2=NEUTRAL\n",
    "- NEUTRAL SENTIMENTS: Balanced opinions, mild preferences, uncertain feelings\n",
    "- QUESTIONS: Inquiries, requests for information, interrogative sentences\n",
    "- ADVERTISEMENTS: Promotional content, sales pitches, marketing messages\n",
    "- MISCELLANEOUS: Greetings, well-wishes, general social interactions\n",
    "\n",
    "final level -\n",
    "this level indicate wahat is overall emotion of a text sentence\\n\n",
    "\n",
    "\n",
    "\n",
    "VALIDATION RULES:\\n\n",
    "if any level label is blank that means you don't have to analyze those level.\\n\n",
    "if language= en that means it's English language\n",
    "\n",
    "INPUT FORMAT:\n",
    "Sentence: [TEXT_TO_ANALYZE]\n",
    "Language: [LANGUAGE_CODE]\n",
    "Level 1 Label: NOISE OR OBJECTIVE OR SUBJECTIVE\n",
    "Level 2 Label: NUETRAL OR NEGATIVE OR POSITIVE OR BLANK\n",
    "Level 3 Label: NUETRAL SENTIMENT OR QUESTION OR ADVERTISEMENT OR MISCELLANEOUS OR BLANK \n",
    "final level: NOISE OR OBJECTIVE OR SUBJECTIVE OR NUETRAL OR NEGATIVE OR POSITIVE OR BLANK OR NUETRAL SENTIMENT OR QUESTION OR ADVERTISEMENT OR MISCELLANEOUS OR BLANK\n",
    "\n",
    "DETAILED EXAMPLES:\n",
    "CORRECT CLASSIFICATIONS:\n",
    "Example 1: NOISE\n",
    "- Sentence: \"asdfgh !@#$%^& \"\n",
    "- Language: en\n",
    "- Level 1: noise, Level 2: [blank], Level 3: [blank]\n",
    "- final level-noise\n",
    "- Expected Output: AGREE\\n\n",
    "\n",
    "Example 2: positive in SUBJECTIVE\n",
    "- Sentence: \"This restaurant series the most amazing pasta I've ever tasted!\"\n",
    "- Language: en\n",
    "- Level 1: subjective, Level 2: positive, Level 3: [blank]\n",
    "final level- positive\n",
    "- Expected Output: AGREE\\n\n",
    "\n",
    "Example 3: Objective mislabeled as Subjective\n",
    "- Sentence: \"Apple Inc. reported quarterly earnings of $89.5 billion.\"\n",
    "- Language: en\n",
    "- Level 1: subjective, Level 2:positive, Level 3: [blank]\n",
    "final level - positive\n",
    "- Expected Output: DISAGREE\\n\n",
    "\n",
    "Example 4: Negative sentiment mislabeled as Positive\n",
    "- Sentence: \"This movie was boring and a complete waste of time.\"\n",
    "- Language: en\n",
    "- Level 1: 2, Level 2: 2, Level 3: [blank]\n",
    "final level - positive\n",
    "- Expected Output: DISAGREE\n",
    "\n",
    "EVALUATION PROCESS:\n",
    "1. Read and understand the sentence in the specified language\n",
    "2. Analyze content type: Is it noise, factual, or opinion-based?\n",
    "3. Assess sentiment: If subjective, determine positive/negative/neutral tone\n",
    "4. Check subcategories: If neutral, identify specific type\n",
    "5. Validate hierarchy: Ensure labels follow the hierarchical rules\n",
    "6. Binary decision: AGREE if all labels are correct, DISAGREE if any label is wrong\\n\n",
    "\n",
    "\n",
    "Now the game starts from here analyze each level of below input answer strictly inAgree or Disagree and strictly follow rule\\n\n",
    ": here is your sentence and each level datatype\\n\n",
    "Sentence: {tweet}\\n\n",
    "Language: {language}\\n\n",
    "Level 1 Label: {label_1}\\n\n",
    "Level 2 Label: {label_2}\\n\n",
    "Level 3 Label: {label_3}\\n\n",
    "final label: {label_final}\\n\n",
    "\n",
    "'''\n",
    "    prompt=base_prompt.format(tweet=tweet,language=language,label_1=label_1,label_2=label_2,label_3=label_3,label_final=label_final)\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a14f648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c7a3f343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # input_text=\"what are macronutrient? and what role they play in human body?\"\n",
    "# # print(f\"input text:{query}\")\n",
    "# dailogue_template=[\n",
    "#     {\n",
    "#         \"role\":\"user\",\n",
    "#         \"content\": query\n",
    "#     }\n",
    "# ]\n",
    "# prompt=tokenizer.apply_chat_template(conversation=dailogue_template,tokenize=False,add_generation_prompt=True)\n",
    "# print(f\"format template:\\n{query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "c8f61b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_id=tokenizer(prompt,return_tensors='pt').to(device)\n",
    "# input_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0d4e4181",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# output=llm_model.generate(**input_id,max_new_tokens=60)\n",
    "# decode_output=tokenizer.decode(output[0])\n",
    "# pattern = r\"<bos><bos><start_of_turn>user.*?<start_of_turn>model\"\n",
    "# cleaned_output = re.sub(pattern, \"\", decode_output, flags=re.DOTALL)\n",
    "# print(f\"output: {cleaned_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ade7407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def Pipeline(total_data_num:int,max_gen_token:int):\n",
    "    query=promt_generator(num=total_data_num,data=data)\n",
    "    dailogue_template=[\n",
    "        {\n",
    "            \"role\":\"user\",\n",
    "            \"content\": query\n",
    "        }\n",
    "    ]\n",
    "    prompt=tokenizer.apply_chat_template(conversation=dailogue_template,tokenize=False,add_generation_prompt=True)\n",
    "    input_id=tokenizer(prompt,return_tensors='pt').to(device)\n",
    "    output=llm_model.generate(**input_id,max_new_tokens=max_gen_token)\n",
    "    decode_output=tokenizer.decode(output[0])\n",
    "    pattern = r\"<bos><bos><start_of_turn>user.*?<start_of_turn>model\"\n",
    "    cleaned_output = re.sub(pattern, \"\", decode_output, flags=re.DOTALL)\n",
    "    print(cleaned_output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe309304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Okay, analyzing the sentence \"i vote for at this year is\"\n",
      "\n",
      "Level 1 Label: Noise\n",
      "Level 2 Label: nan\n",
      "Level 3 Label: nan<end_of_turn>\n",
      "\n",
      "Okay, let's analyze the provided sentence.\n",
      "\n",
      "Sentence: start with what is right rather than what is acceptable. -franz kafka\n",
      "Language: en\n",
      "Level 1 Label: Noise\n",
      "Level 2 Label: nan\n",
      "Level 3 Label: nan\n",
      "\n",
      "Final Label: Noise\n",
      "<end_of_turn>\n",
      "\n",
      "Okay, here's my analysis of the provided sentence:\n",
      "\n",
      "Sentence: \"ain the same day i saw & i had an autograph i votea\"\n",
      "Language: en\n",
      "Level 1 Label: Noise\n",
      "Level 2 Label: NaN\n",
      "Level 3 Label: NaN\n",
      "final label:\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    Pipeline(total_data_num=i,max_gen_token=512)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
